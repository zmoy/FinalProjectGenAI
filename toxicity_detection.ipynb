{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Final Project\n",
    "\n",
    "## Creation of a Machine Learning System dedicated to Toxicity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "By completing this project, you will:\n",
    "• Design and implement a complete ML system for text classification, from model creation\n",
    "to deployment\n",
    "• Get a first glimpse of what is called MLOps\n",
    "• Consider ethical implications in ML deployment\n",
    "• Work on a complex topic that hasn't been solved yet!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project tasks\n",
    "\n",
    "This project revolves around 2 key aspects:\n",
    "\n",
    "1. 2. Creation of a machine learning model dedicated to toxicity detection in texts\n",
    "Exploration of other components required to create a full-fledge ML system. As such, the content of the project consists in the following tasks:\n",
    "\n",
    "1. 2. 3. Define the metrics you'll use to evaluate the quality of your models\n",
    "Create a data preprocessing pipeline to prepare the texts for model training\n",
    "Use a LLM to create a baseline out of your test set\n",
    "\n",
    "a. You can experiment with multiple LLMs but don't spend too much time on complex prompting experiments: the goal is to have a quick baseline to beat / reach and see how training a custom model compares to it\n",
    "b. If you don't have a GPU on your own machines such inference will likely take time. You can use Google Collab free GPU notebooks to speed up the inference. 7B / 4B models should run fine if quantized4. 5. 6. 7. c. If that still takes too much time, consider creating a sample out of your testing data. \n",
    "\n",
    "In that case, use a seed to ensure reproductibility and use the same test sample to compare the performance of your trained model(s) later on Finetune the RoBERTa-base model using the training data attached to this project\n",
    "a. This step involves hyperparameter optimization, but you don't need to use PBT\n",
    "optimization approaches seen in class\n",
    "b. You will need to create a Model Card for your finetuned model\n",
    "\n",
    "Question :\n",
    "• Conduct a bias / fairness assessment of your model\n",
    "o What do we call bias / fairness in ML systems, specifically for texts?\n",
    "o Measure how much your model is sensible to identified bias\n",
    "\n",
    "• Estimate how much your baseline / finetuned model is resilient to obfuscation / jailbreak\n",
    "o Explore how you can mitigate jailbreaking / obfuscation attempts of NLP system\n",
    "o Implement some approaches in the API that expose your model\n",
    "o Note: we're talking here of jailbreaking a model, not the API itself. As such,\n",
    "considerations like authentication / networking security shouldn't be taken into\n",
    "account.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
